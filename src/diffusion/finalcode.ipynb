{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b93195",
   "metadata": {},
   "source": [
    "### 이 코드는 연구의 전반부 목적인 논문 재현까지의 흐름을 담고 있습니다. \n",
    "### 해당 코드의 결과를 통해 ddpm을 활용한 data augmentation의 Inverse Ising Inference 에서의 추론 성능 향상을 확인하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import scipy as sp\n",
    "from time import perf_counter\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from numba import jit\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e13ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용하고자 하는 ddpm 스케줄링을 미리 정의합니다.\n",
    "\n",
    "def f(t, T, s=0.008):\n",
    "    return torch.cos(((t / T) + s) / (1 + s) * torch.pi / 2) ** 2\n",
    "def beta_schedule(beta1, beta2, T, schedule='cosine'):\n",
    "    if schedule == 'linear':\n",
    "        # Linear schedule: linearly increase beta from beta1 to beta2\n",
    "        betas = torch.linspace(beta1, beta2, T)\n",
    "\n",
    "    elif schedule == 'cosine':\n",
    "        # Cosine schedule\n",
    "        s = 0.008\n",
    "        timesteps = torch.arange(0, T + 1)  # t from 0 to T\n",
    "        f_t = f(timesteps, T, s)  # f(t) for t=0 to T\n",
    "        \n",
    "        bar_alpha = f_t / f_t[0]\n",
    "        \n",
    "        # Initialize betas tensor\n",
    "        betas = torch.zeros(T)\n",
    "        \n",
    "        # Compute betas using the given formula\n",
    "        for t in range(1, T + 1):\n",
    "            beta = 1 - (bar_alpha[t] / bar_alpha[t - 1])\n",
    "            beta = torch.clamp(beta, min=0.0, max=0.999)\n",
    "            betas[t - 1] = beta\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Schedule must be either 'linear' or 'cosine'.\")\n",
    "    \n",
    "    return betas\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T, schedule='cosine'):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert 0 < beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "    \n",
    "    beta_t = beta_schedule(beta1, beta2, T, schedule)\n",
    "    \n",
    "    sqrt_beta_t = torch.sqrt(beta_t)                    # \\sqrt{\\beta_t}\n",
    "    alpha_t = 1 - beta_t                                 # \\alpha_t\n",
    "    alphabar_t = torch.cumprod(alpha_t, dim=0)           # \\bar{\\alpha_t}\n",
    "    sqrtab = torch.sqrt(alphabar_t)                      # \\sqrt{\\bar{\\alpha_t}}\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)              # 1/\\sqrt{\\alpha_t}\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)                 # \\sqrt{1 - \\bar{\\alpha_t}}\n",
    "    bt_over_sqrtmab = beta_t / sqrtmab                   # \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\n",
    "    \n",
    "    return {\n",
    "        \"beta_t\": beta_t,                          # \\beta_t\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,                # \\sqrt{\\beta_t}\n",
    "        \"alpha_t\": alpha_t,                        # \\alpha_t\n",
    "        \"alphabar_t\": alphabar_t,                  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,                          # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"oneover_sqrta\": oneover_sqrta,            # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrtmab\": sqrtmab,                        # \\sqrt{1 - \\bar{\\alpha_t}}\n",
    "        \"bt_over_sqrtmab\": bt_over_sqrtmab,        # \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ac6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ising 모델의 샘플을 생성하여 ddpm의 data augmentation 결과와 비교할 기본 데이터셋을 만들기 위해, MCMC 알고리즘을 정의합니다.\n",
    "\n",
    "# Metropolis Algorithm\n",
    "\n",
    "# decide whether to flip a chosen spin or not\n",
    "@jit(nopython=True,cache=True)\n",
    "def Metro(seq,i,h,J):\n",
    "    # seq: (sigma_1, sigma_2, ..., sigma_N), sigma = 1 or -1\n",
    "    # i: chosen spin\n",
    "    \n",
    "    dE = 2*seq[i]*(h[i]+np.dot(J[i,:],seq))\n",
    "    if dE<0:\n",
    "        seq[i] = -seq[i]\n",
    "    else:\n",
    "        prob = np.exp(-dE)\n",
    "        if np.random.rand() < prob:\n",
    "            seq[i] = -seq[i]\n",
    "    return seq\n",
    "\n",
    "# Markov chain Monte Carlo\n",
    "def MCMC_nonparallel(seed,L,N,h,J):\n",
    "    # we do not use \"seed\" here\n",
    "    # L: sample size, N: system size, h & J: Ising parameters\n",
    "    \n",
    "    seq = np.random.choice(np.array([+1.0,-1.0]),N) # initial random sequence\n",
    "    \n",
    "    iset = np.arange(N)\n",
    "    \n",
    "    # thermalizing step\n",
    "    @jit(nopython=True)\n",
    "    def thermalize(seq):\n",
    "        nstep = N*10000 # run N*5000 Metropolis steps\n",
    "        for step in range(nstep):\n",
    "            i = np.random.choice(iset)\n",
    "            seq = Metro(seq,i,h,J)\n",
    "        return seq\n",
    "    \n",
    "    seq = thermalize(seq)\n",
    "    \n",
    "    # sampling step\n",
    "    @jit(nopython=True)\n",
    "    def sampling(seq):\n",
    "        s = np.zeros((L,N)) # sampling data\n",
    "        interval = N*100 # sample a sequence per each N*50 steps\n",
    "        nstep = interval*(L-1)+1\n",
    "        ind = 0\n",
    "        for step in range(nstep):\n",
    "            i = np.random.choice(iset)\n",
    "            seq = Metro(seq,i,h,J)\n",
    "            if step%interval==0:\n",
    "                s[ind,:] = seq\n",
    "                ind += 1\n",
    "        return s\n",
    "    \n",
    "    s = sampling(seq)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Markov chain Monte Carlo with parallel computing\n",
    "def MCMC(L,N,h,J,n_par=1):\n",
    "    # n_par: number of parallel cores\n",
    "    \n",
    "    n_par = int(n_par)\n",
    "    \n",
    "    if n_par==1: # no parallel computing\n",
    "        return MCMC_nonparallel(L=L,N=N,h=h,J=J,seed=0)\n",
    "    \n",
    "    else: # run parallel computing\n",
    "        pool = multiprocessing.Pool(processes=n_par)\n",
    "        MCMC_individual = partial(MCMC_nonparallel,L=int(L/n_par)+1,N=N,h=h,J=J)\n",
    "        s_list = pool.map(MCMC_individual,range(n_par))\n",
    "        pool.close()\n",
    "        return np.concatenate(s_list,axis=0)[:L,:]\n",
    "    \n",
    "@jit(nopython=True,cache=True)\n",
    "def statistics(s):\n",
    "    # return <sigma_i> and <sigma_i * sigma_j>\n",
    "    \n",
    "    L, N = s.shape # L: sample size, N: system size\n",
    "    \n",
    "    m = np.sum(s,axis=0)/L # mean activity\n",
    "    m2 = s.T@s/L # <sigma_i * sigma_j>\n",
    "    \n",
    "    for i in range(N):\n",
    "        m2[i,i] = 0\n",
    "    \n",
    "    return m, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operator로 연산하기 위해 함수를 정의합니다.\n",
    "\n",
    "# convert s[n_seq,n_var] to ops[n_seq,n_ops]\n",
    "def operators(s):\n",
    "    #generate terms in the energy function\n",
    "    n_seq,n_var = s.shape\n",
    "    ops = np.zeros((n_seq,n_var+int(n_var*(n_var-1)/2.0)))\n",
    "\n",
    "    jindex = 0\n",
    "    for index in range(n_var):\n",
    "        ops[:,jindex] = s[:,index]\n",
    "        jindex +=1\n",
    "\n",
    "    for index in range(n_var-1):\n",
    "        for index1 in range(index+1,n_var):\n",
    "            ops[:,jindex] = s[:,index]*s[:,index1]\n",
    "            jindex +=1\n",
    "\n",
    "    return ops\n",
    "\n",
    "def energy_ops(ops,J):\n",
    "    return np.sum(ops*J[np.newaxis,:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab022867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Ising Inference를 위해 erasure machine을 방법론으로 채택하고 이를 구현합니다. \n",
    "# erasure machine\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_energy(seq, h, J):\n",
    "    field_term = -np.dot(h, seq)\n",
    "    interaction_term = -np.sum(np.triu(J, k=1) * np.outer(seq, seq))\n",
    "    return field_term + interaction_term\n",
    "\n",
    "def estimate_f_epsilon(samples, h, J, epsilon):\n",
    "    energies = np.array([compute_energy(s, h, J) for s in samples])\n",
    "    ps = np.exp(-energies)\n",
    "    ps_epsilon = ps ** (-1 + epsilon)\n",
    "    weights = ps_epsilon / np.sum(ps_epsilon)\n",
    "    return weights\n",
    "\n",
    "def update_parameters(samples, h, J, epsilon, lr):\n",
    "    n = samples.shape[1]\n",
    "    weights = estimate_f_epsilon(samples, h, J, epsilon)\n",
    "\n",
    "    mean_sigma = np.zeros(n)\n",
    "    for s, wgt in zip(samples, weights):\n",
    "        mean_sigma += wgt * s\n",
    "    #mean_sigma = np.average(samples, axis=0, weights=weights)  # ⟨σ_i⟩_{f_ε}\n",
    "    mean_sigma_sigma = np.zeros((n, n))\n",
    "    for s, wgt in zip(samples, weights):\n",
    "        #mean_sigma_sigma += wgt * np.einsum(\"i,j->ij\", s, s)\n",
    "        mean_sigma_sigma += wgt * np.outer(s, s)\n",
    "    np.fill_diagonal(mean_sigma_sigma, 0)\n",
    "    h_model = epsilon * h\n",
    "    J_model = epsilon * J\n",
    "\n",
    "    #print(np.linalg.norm(mean_sigma - h_model))\n",
    "    \n",
    "    h_new = h + lr * (mean_sigma - h_model)\n",
    "    J_new = J + lr * (mean_sigma_sigma - J_model)\n",
    "    \n",
    "    return h_new, J_new\n",
    "\n",
    "def train_erasure_machine(samples, n, epsilon=0.05, lr=0.1, epochs=100):\n",
    "    h_temp = np.random.randn(n) * 0.01\n",
    "    J_temp = np.random.randn(n, n) * 0.01\n",
    "    J_temp = (J_temp + J_temp.T) / 2  # Make symmetric\n",
    "    np.fill_diagonal(J_temp, 0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        h_temp, J_temp = update_parameters(samples, h_temp, J_temp, epsilon, lr)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: h norm = {np.linalg.norm(h_temp):.4f}, J norm = {np.linalg.norm(J_temp):.4f}\")\n",
    "    return h_temp, J_temp\n",
    "\n",
    "# erasure machine을 테스트해본 결과, \n",
    "# s=[샘플수, 모델 차원], L은 모델 차원(40), 엡실론은 0.7 정도일 때 결과가 가장 잘 나왔고 에폭도 150 정도에서 잘 수렴하는 것을 확인해,\n",
    "# 에폭 150, 엡실론 0.7을 이후 연구에서도 활용하고자 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7868cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation의 III 효과를 보기 위한 실험 설계의 구체적 과정입니다. \n",
    "# 먼저 MCMC로 Ising 모델의 샘플을 생성하고, 이를 DDPM 모델 학습에 활용할 수 있는 형태로 변환합니다.\n",
    "# 그리고 DDPM 모델을 학습시켜 MCMC 샘플만으로 III를 진행했을 때의 성능과,\n",
    "# DDPM 모델이 생성한 샘플을 augmentation에 활용했을 때의 성능을 MSE와 Var[E] 그래프로 비교합니다.\n",
    "\n",
    "# 1. MCMC 샘플 생성\n",
    "N = 40  # 시스템 크기\n",
    "L = 4000  # 샘플 개수\n",
    "\n",
    "# generate samples\n",
    "g_h = 0.2 # scale for h\n",
    "g_J = 1.0 # scale for J\n",
    "\n",
    "h_true = np.random.normal(0.0,g_h,N) # external bias\n",
    "J_true = np.random.normal(0.0,g_J/np.sqrt(N),(N,N)) # coupling\n",
    "\n",
    "np.save(\"ising_h_real.npy\", h_true)\n",
    "np.save(\"ising_J_real.npy\", J_true)\n",
    "\n",
    "# symmetrize the coupling\n",
    "for i in range(N):\n",
    "    J_true[i,i] = 0\n",
    "    \n",
    "for i in range(N-1):\n",
    "    for j in range(i+1,N):\n",
    "        J_true[i,j] = J_true[j,i]\n",
    "    \n",
    "\n",
    "# 기존 MCMC 함수 사용\n",
    "s = np.load(\"../../sunjae/ising_samples.npy\")\n",
    "\n",
    "# PyTorch Tensor 변환\n",
    "s_tensor = torch.from_numpy(s).float()  # (L, N) 크기\n",
    "print(f\"Shape of s_tensor: {s_tensor.shape}\")  # (5000, 40)\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = 256\n",
    "dataset = TensorDataset(s_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# mcmc의 에너지 분포를 시각화하기 위한 함수\n",
    "def plot_energy_distribution(s_tensor, h, J, bins=10, title=\"Energy Distribution\"):\n",
    "    \"\"\"\n",
    "    s_tensor: (L, N) torch tensor\n",
    "    h_true: (N,) numpy array\n",
    "    J_true: (N, N) numpy array\n",
    "    \"\"\"\n",
    "    # h, J를 Torch Tensor로 변환\n",
    "    h_torch = torch.from_numpy(h_true).float().to(s_tensor.device)\n",
    "    J_torch = torch.from_numpy(J_true).float().to(s_tensor.device)\n",
    "\n",
    "    # 에너지 계산\n",
    "    E_h = -torch.matmul(s_tensor, h_torch)  # (L,)\n",
    "    E_J = -torch.einsum('bi,ij,bj->b', s_tensor, J_torch, s_tensor) / 2\n",
    "    E = E_h + E_J  # (L,)\n",
    "\n",
    "    E_numpy = E.detach().cpu().numpy()\n",
    "\n",
    "    # 플랏으로 에너지 분포 확인\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(E_numpy, bins=bins, color='lightcoral', edgecolor='black', alpha=0.8)\n",
    "    plt.xlabel(\"Energy of mcmc samples, M=4000\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show('energy_distribution.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 통계 출력\n",
    "    print(f\"Mean Energy: {np.mean(E_numpy):.4f}\")\n",
    "    print(f\"Std Energy: {np.std(E_numpy):.4f}\")\n",
    "    print(f\"Min Energy: {np.min(E_numpy):.4f}\")\n",
    "    print(f\"Max Energy: {np.max(E_numpy):.4f}\")\n",
    "\n",
    "    return E_numpy\n",
    "\n",
    "E_numpy = plot_energy_distribution(s_tensor, h_true, J_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. M=4000으로 DDPM 모델 학습하기 위해 DDPM 모델 정의\n",
    "# 아래는 구체적인 DDPM 모델 정의와 학습을 위한 코드입니다.\n",
    "\n",
    "# -------------------------------\n",
    "# 1 Sinusoidal Time Embedding\n",
    "# -------------------------------\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "# -------------------------------\n",
    "# 2️ Diffusion MLP (논문 구조 재현)\n",
    "# -------------------------------\n",
    "class DiffusionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=40):\n",
    "        super().__init__()\n",
    "        time_embed_dim = 128\n",
    "        self.time_embed = SinusoidalTimeEmbedding(time_embed_dim)\n",
    "\n",
    "        self.enc1 = nn.Linear(input_dim + time_embed_dim, 128)\n",
    "        self.enc2 = nn.Linear(128, 256)\n",
    "        self.enc3 = nn.Linear(256, 512)\n",
    "\n",
    "        self.dec1 = nn.Linear(512, 512)\n",
    "        self.dec2 = nn.Linear(512, 512)\n",
    "        self.dec3 = nn.Linear(512, 256)\n",
    "        self.dec4 = nn.Linear(256, 128)\n",
    "\n",
    "        self.out = nn.Linear(128, input_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.time_embed(t).to(x.dtype)\n",
    "        h = torch.cat([x, t_embed], dim=1)\n",
    "\n",
    "        h1 = self.act(self.enc1(h))\n",
    "        h2 = self.act(self.enc2(h1))\n",
    "        h3 = self.act(self.enc3(h2))\n",
    "\n",
    "        d1 = self.act(self.dec1(h3))\n",
    "        d2 = self.act(self.dec2(d1) + d1)\n",
    "        d3 = self.act(self.dec3(d2) + h2)\n",
    "        d4 = self.act(self.dec4(d3) + h1)\n",
    "\n",
    "        return self.out(d4)\n",
    "\n",
    "# -------------------------------\n",
    "# 3 DDPM Schedules\n",
    "# -------------------------------\n",
    "def ddpm_schedules(beta_start, beta_end, n_T):\n",
    "    betas = torch.linspace(beta_start, beta_end, n_T)\n",
    "    alphas = 1. - betas\n",
    "    alphabar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return {\n",
    "        'beta_t': betas,\n",
    "        'alpha_t': alphas,\n",
    "        'alphabar_t': alphabar,\n",
    "        'sqrtab': torch.sqrt(alphabar),\n",
    "        'sqrtmab': torch.sqrt(1 - alphabar),\n",
    "        'oneover_sqrta': 1. / torch.sqrt(alphas),\n",
    "        'bt_over_sqrtmab': betas / torch.sqrt(1 - alphabar),\n",
    "        'sqrt_beta_t': torch.sqrt(betas),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 4️ DDPM Class (Device 문제 해결)\n",
    "# -------------------------------\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device):\n",
    "        super().__init__()\n",
    "        self.nn_model = nn_model\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.to(device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        t = torch.randint(1, self.n_T+1, (batch_size,), device=x.device).float()\n",
    "\n",
    "        sqrtab_t = self.sqrtab[t.long()-1].view(batch_size, 1)\n",
    "        sqrtmab_t = self.sqrtmab[t.long()-1].view(batch_size, 1)\n",
    "\n",
    "        epsilon = torch.randn_like(x)\n",
    "        x_t = sqrtab_t * x + sqrtmab_t * epsilon\n",
    "\n",
    "        pred_epsilon = self.nn_model(x_t, t)\n",
    "        loss = F.mse_loss(pred_epsilon, epsilon)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, n_sample, size):\n",
    "        x = torch.randn(n_sample, size).to(self.device)\n",
    "        x_seq = []\n",
    "\n",
    "        for t in reversed(range(1, self.n_T+1)):\n",
    "            t_batch = torch.full((n_sample,), t, device=self.device).float()\n",
    "            epsilon_pred = self.nn_model(x, t_batch)\n",
    "\n",
    "            mu = self.oneover_sqrta[t-1] * (x - self.bt_over_sqrtmab[t-1] * epsilon_pred)\n",
    "            sigma = self.sqrt_beta_t[t-1]\n",
    "\n",
    "            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
    "            x = mu + sigma * z\n",
    "\n",
    "            if t % (self.n_T // 10) == 0 or t == 1:\n",
    "                x_seq.append(x.detach().cpu().numpy())\n",
    "\n",
    "        return x, np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델 학습 및 데이터 생성 \n",
    "\n",
    "# -------------------------------------\n",
    "# 5 DDPM Training and Data Generation\n",
    "# -------------------------------------\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 논문의 MSE 계산 함수 재현\n",
    "def mse_paper(h_learned, J_learned, h_true, J_true):\n",
    "    N = h_true.shape[0]\n",
    "    mask = np.triu(np.ones((N, N)), k=1).astype(bool)\n",
    "    mse_h = np.mean((h_learned - h_true) ** 2)\n",
    "    mse_J = np.mean((J_learned[mask] - J_true[mask]) ** 2)\n",
    "    return mse_h + mse_J\n",
    "\n",
    "# DDPM Model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DiffusionMLP(input_dim=N)\n",
    "ddpm = DDPM(model, betas=(0.0001, 0.02), n_T=1000, device=device)\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=1e-4)\n",
    "\n",
    "max_iterations = 50000\n",
    "loss_list = []\n",
    "mse_list = []\n",
    "varE_list = []\n",
    "iteration = 0\n",
    "save_every = 1000\n",
    "\n",
    "dataloader_iter = iter(dataloader)\n",
    "while iteration < max_iterations:\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "    except StopIteration:\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        batch = next(dataloader_iter)\n",
    "\n",
    "    x = batch[0].to(device)\n",
    "    loss = ddpm(x)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    avg_loss = loss.item()\n",
    "    loss_list.append((iteration + 1, avg_loss))\n",
    "\n",
    "    # Print progress\n",
    "    if (iteration + 1) % 100 == 0 or (iteration + 1) == max_iterations:\n",
    "        print(f\"Iteration {iteration+1}/{max_iterations}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Every 1000 iterations, generate samples and compute Var[E] and MSE\n",
    "    if (iteration + 1) % 1000 == 0 or (iteration + 1) == max_iterations:\n",
    "        with torch.no_grad():\n",
    "            n_sample = 100_000\n",
    "            x_diff, _ = ddpm.sample(n_sample, size=N)\n",
    "        x_diff = np.sign(x_diff.cpu().numpy())\n",
    "\n",
    "        # Var[E] for DDPM samples\n",
    "        E_diff = np.array([compute_energy(seq, h_true, J_true) for seq in x_diff])\n",
    "        varE = np.var(E_diff)\n",
    "        varE_list.append((iteration + 1, varE))\n",
    "        print(f\"Var[E_diff]: {varE:.6f}\")\n",
    "\n",
    "        # MSE for DDPM samples (augmented data) using erasure machine\n",
    "        h_learned, J_learned = train_erasure_machine(x_diff, N, epsilon=0.7, epochs=150)\n",
    "        mse = mse_paper(h_learned, J_learned, h_true, J_true)\n",
    "        mse_list.append((iteration + 1, mse))\n",
    "        print(f\"MSE (augmented) at iter {iteration+1}: {mse:.6f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"checkpoint_iter{iteration+1}.pt\")\n",
    "\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 결과 분석\n",
    "# -------------------------------\n",
    "# 6 Plotting and Saving Results\n",
    "# -------------------------------\n",
    "\n",
    "# MSE 및 Var[E] plot\n",
    "\n",
    "# MSE for original MCMC samples (obs(train))\n",
    "h_mcmc, J_mcmc = train_erasure_machine(s, N, epsilon=0.7, epochs=150)\n",
    "mse_train = mse_paper(h_mcmc, J_mcmc, h_true, J_true)\n",
    "\n",
    "# Var[E] for original MCMC samples (obs(train))\n",
    "E_mcmc = np.array([compute_energy(seq, h_true, J_true) for seq in s])\n",
    "varE_train = np.var(E_mcmc)\n",
    "\n",
    "# Plotting the results\n",
    "\n",
    "# MSE plot\n",
    "iters, mses = zip(*mse_list)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.array(iters)/1000, mses, label=\"aug\", color='black')\n",
    "plt.axhline(mse_train, color='black', linestyle='--', label=\"obs (train)\")\n",
    "plt.xlabel(\"Iteration (K)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE vs Iteration\")\n",
    "plt.grid(alpha=0.1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show(\"mse_paper_style.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Var[E] plot\n",
    "iters, varEs = zip(*varE_list)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.array(iters)/1000, varEs, label=\"aug\", color='black')\n",
    "plt.axhline(varE_train, color='black', linestyle='--', label=\"obs (train)\")\n",
    "plt.xlabel(\"Iteration (K)\")\n",
    "plt.ylabel(\"Var[E]\")\n",
    "plt.title(\"Energy Variance vs Iteration\")\n",
    "plt.grid(alpha=0.1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show(\"varE_paper_style.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 결과 재현 및 III 구현을 위해 생성된 샘플 및 parameter를 npy로 저장\n",
    "np.save(\"diffusion_samples_real.npy\", x_diff)\n",
    "np.save(\"mcmc_samples_real.npy\", s)\n",
    "np.save(\"mse_list.npy\", np.array(mse_list))\n",
    "np.save(\"varE_list.npy\", np.array(varE_list))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
