{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde7ed52",
   "metadata": {},
   "source": [
    "If you need tools to run Inverse ising inference with different approach, you can use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb88e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy_torch(samples, b, w):\n",
    "    # energy = -b·σ - sum_{i<j} w_ij σ_i σ_j\n",
    "    field_term = -torch.matmul(samples, b)\n",
    "    interaction_term = -0.5 * torch.sum(samples @ w * samples, dim=1)\n",
    "    return field_term + interaction_term\n",
    "\n",
    "def estimate_f_epsilon_torch(samples, b, w, epsilon):\n",
    "    with torch.no_grad():\n",
    "        energies = compute_energy_torch(samples, b, w)\n",
    "        ps = torch.exp(-energies)\n",
    "        ps_epsilon = ps ** (-1 + epsilon)\n",
    "        weights = ps_epsilon / ps_epsilon.sum()\n",
    "    return weights\n",
    "\n",
    "def update_parameters_torch(samples, b, w, epsilon, lr):\n",
    "    weights = estimate_f_epsilon_torch(samples, b, w, epsilon)\n",
    "\n",
    "    mean_sigma = torch.sum(weights[:, None] * samples, dim=0)\n",
    "\n",
    "    outer = samples.unsqueeze(2) * samples.unsqueeze(1)  # shape: (N, L, L)\n",
    "    weighted_outer = weights[:, None, None] * outer\n",
    "    mean_sigma_sigma = torch.sum(weighted_outer, dim=0)\n",
    "    mean_sigma_sigma.fill_diagonal_(0.0)\n",
    "\n",
    "    b_model = epsilon * b\n",
    "    w_model = epsilon * w\n",
    "\n",
    "    b_new = b + lr * (mean_sigma - b_model)\n",
    "    w_new = w + lr * (mean_sigma_sigma - w_model)\n",
    "\n",
    "    return b_new, w_new\n",
    "\n",
    "def train_erasure_machine_torch(samples_np, n, epsilon=0.05, lr=0.1, epochs=100, use_cuda=True):\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    samples = torch.tensor(samples_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    b = torch.randn(n, device=device) * 0.01\n",
    "    w = torch.randn(n, n, device=device) * 0.01\n",
    "    w = (w + w.T) / 2\n",
    "    w.fill_diagonal_(0.0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        b, w = update_parameters_torch(samples, b, w, epsilon, lr)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[GPU] Epoch {epoch}: b norm = {b.norm():.4f}, w norm = {w.norm():.4f}\")\n",
    "    return b.cpu().numpy(), w.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408c5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_ising_logistic(data):\n",
    "    \"\"\"\n",
    "    data: M x n numpy array of spin configurations (+1 or -1)\n",
    "    Returns: biases b (n,), coupling matrix W (n x n)\n",
    "    \"\"\"\n",
    "    M, n = data.shape\n",
    "    W = np.zeros((n, n))\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        y = data[:, i]\n",
    "\n",
    "        X = np.delete(data, i, axis=1)\n",
    "\n",
    "        y_binary = (y + 1) // 2\n",
    "\n",
    "        clf = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "        clf.fit(X, y_binary)\n",
    "\n",
    "        b[i] = clf.intercept_[0] / 2.0\n",
    "        w_i = clf.coef_[0] / 2.0\n",
    "        W[i, np.arange(n) != i] = w_i\n",
    "\n",
    "    W = (W + W.T) / 2.0\n",
    "    return b, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16526f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ac7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w, w_e, b, b_e) :\n",
    "    LL = L + L*(L-1)/2\n",
    "    b_error = np.sum((b_e - b)**2)/LL\n",
    "    mask = np.triu(np.ones_like(w), k=1)\n",
    "    w_diff = (w_e - w) * mask\n",
    "    w_error = np.sum(w_diff**2)/LL\n",
    "    return (b_error + w_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
